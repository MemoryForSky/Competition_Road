{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zh\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 读取数据，简单处理list数据\n",
    "train = pd.read_csv('./data/train.txt', header=None)\n",
    "test = pd.read_csv('./data/apply_new.txt', header=None)\n",
    "\n",
    "train.columns = ['pid', 'label', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "test.columns = ['pid', 'gender', 'age', 'tagid', 'time', 'province', 'city', 'model', 'make']\n",
    "\n",
    "train['label'] = train['label'].astype(int)\n",
    "\n",
    "data = pd.concat([train,test])\n",
    "data['label'] = data['label'].fillna(-1)\n",
    "\n",
    "data['tagid'] = data['tagid'].apply(lambda x:eval(x))\n",
    "data['tagid'] = data['tagid'].apply(lambda x:[str(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 230638 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# 超参数\n",
    "# embed_size  embedding size\n",
    "# MAX_NB_WORDS  tagid中的单词出现次数\n",
    "# MAX_SEQUENCE_LENGTH  输入tagid list的长度\n",
    "embed_size = 64\n",
    "MAX_NB_WORDS = 230637\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "# 训练word2vec，这里可以考虑elmo，bert等预训练\n",
    "w2v_model = Word2Vec(sentences=data['tagid'].tolist(), vector_size=embed_size, window=5, min_count=1,epochs=10)\n",
    "# 这里是划分训练集和测试数据\n",
    "X_train = data[:train.shape[0]]['tagid']\n",
    "X_test = data[train.shape[0]:]['tagid']\n",
    "\n",
    "# 创建词典，利用了tf.keras的API，其实就是编码一下，具体可以看看API的使用方法\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "word_index = tokenizer.word_index\n",
    "# 计算一共出现了多少个单词，其实MAX_NB_WORDS我直接就用了这个数据\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "print('Total %s word vectors.' % nb_words)\n",
    "# 构建一个embedding的矩阵，之后输入到模型使用\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv.get_vector(word)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "y_categorical = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    embedding_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = Embedding(nb_words,\n",
    "                         embed_size,\n",
    "                         input_length=MAX_SEQUENCE_LENGTH,\n",
    "                         weights=[embedding_matrix],\n",
    "                         trainable=False)\n",
    "    embed = embedder(embedding_input)\n",
    "    l = LSTM(128)(embed)\n",
    "    flat = BatchNormalization()(l)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(1, activation='sigmoid')(drop)\n",
    "    model = Model(inputs=embedding_input, outputs=main_output)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 五折交叉验证\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "oof = np.zeros([len(train), 1])\n",
    "predictions = np.zeros([len(test), 1])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['label'])):\n",
    "    print(\"fold n{}\".format(fold_ + 1))\n",
    "    model = my_model()\n",
    "    if fold_ == 0:\n",
    "        model.summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    bst_model_path = \"./{}.h5\".format(fold_)\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    X_tra, X_val = X_train[trn_idx], X_train[val_idx]\n",
    "    y_tra, y_val = y_categorical[trn_idx], y_categorical[val_idx]\n",
    "\n",
    "    model.fit(X_tra, y_tra,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=128, batch_size=256, shuffle=True,\n",
    "              callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    model.load_weights(bst_model_path)\n",
    "\n",
    "    oof[val_idx] = model.predict(X_val)\n",
    "\n",
    "    predictions += model.predict(X_test) / folds.n_splits\n",
    "    print(predictions)\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['predict'] = oof\n",
    "train['rank'] = train['predict'].rank()\n",
    "train['p'] = 1\n",
    "train.loc[train['rank'] <= train.shape[0] * 0.5, 'p'] = 0\n",
    "bst_f1_tmp = f1_score(train['label'].values, train['p'].values)\n",
    "print(bst_f1_tmp)\n",
    "\n",
    "submit = test[['pid']]\n",
    "submit['tmp'] = predictions\n",
    "submit.columns = ['user_id', 'tmp']\n",
    "\n",
    "submit['rank'] = submit['tmp'].rank()\n",
    "submit['category_id'] = 1\n",
    "submit.loc[submit['rank'] <= int(submit.shape[0] * 0.5), 'category_id'] = 0\n",
    "\n",
    "print(submit['category_id'].mean())\n",
    "\n",
    "submit[['user_id', 'category_id']].to_csv('submission_{}.csv'.format(str(bst_f1_tmp).split('.')[1]), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
