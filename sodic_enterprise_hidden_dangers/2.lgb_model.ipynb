{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_rows', 100)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import jieba\n",
    "import jieba.posseg as pseg \n",
    "import lightgbm as lgb\n",
    "# 导入编码转换模块\n",
    "import codecs\n",
    "# 从textrank4zh模块中导入提取关键词和生成摘要的类\n",
    "from textrank4zh import TextRank4Keyword, TextRank4Sentence, util\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt import Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxSame(a, b):\n",
    "    # 获得a,b字符串最大相同子序列\n",
    "    if len(a)<=len(b):\n",
    "        s1, s2 = a, b\n",
    "    else:\n",
    "        s1, s2 = b, a\n",
    "    maxlen = 0\n",
    "    maxstr = \"\"\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a), i-1, -1):\n",
    "            if s1[i:j] in s2 and len(s1[i:j]) > maxlen:\n",
    "                maxlen = len(s1[i:j])\n",
    "                maxstr = s1[i:j]\n",
    "    return maxstr\n",
    "\n",
    "def get_len(x):\n",
    "    x_split = re.split(\"[,，;；.。]\", x)\n",
    "    x_split = [i for i in x_split if len(i)>2]\n",
    "    return len(x_split)\n",
    "\n",
    "def getIndex(a, b):\n",
    "    # 输出a在b中的索引，若a不在b中，则返回-1\n",
    "    if a in b:\n",
    "        return b.index(a)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def get_word_weight(text, nums=100, window=2, speech_tags=['a', 'd', 't', 'v', 'l']):\n",
    "    # 创建分词类的实例\n",
    "    tr4w = TextRank4Keyword(allow_speech_tags=speech_tags)\n",
    "    # 对文本进行分析，设定窗口大小为2，并将英文单词小写\n",
    "    tr4w.analyze(text=text, lower=True, window=window)\n",
    "\n",
    "    # 从关键词列表中获取前20个关键词\n",
    "    word_lst = []\n",
    "    weight_lst = []\n",
    "    for item in tr4w.get_keywords(num=nums, word_min_len=1):\n",
    "        # 打印每个关键词的内容及关键词的权重\n",
    "        word_lst.append(item.word)\n",
    "        weight_lst.append(item.weight)\n",
    "    word_weight = pd.DataFrame({'word': word_lst, 'weight': weight_lst})\n",
    "    return word_weight\n",
    "    \n",
    "def get_level_content_features(df):\n",
    "    \"\"\"level和content的相关类特征\"\"\"\n",
    "    # level_4和content只保留中文文本\n",
    "    df[\"content\"] = df[\"content\"].astype(str)\n",
    "    regex = re.compile(u\"[\\u4e00-\\u9fa5]+\")\n",
    "    df[\"level4_nosign\"] = df[\"level_4\"].apply(lambda x: ''.join(regex.findall(x)))\n",
    "    df[\"content_nosign\"] = df[\"content\"].apply(lambda x: ''.join(regex.findall(x)))\n",
    "    \n",
    "    #------------------------- 最长公共子序列 -------------------------#\n",
    "    # content是否包含于level_4\n",
    "    df[\"content_in_level4\"] = df.apply(lambda x: int(x.content_nosign in x.level4_nosign), axis=1)\n",
    "    # content与level4最长相同子字符串\n",
    "    df[\"content_level4_substr\"] = df.apply(lambda x: getMaxSame(x.content_nosign, x.level4_nosign), axis=1)\n",
    "    # content与level4最长相同子字符串长度\n",
    "    df[\"content_level4_sublen\"] = df[\"content_level4_substr\"].str.len()\n",
    "    df[\"level4_strlen\"] = df.level_4.apply(len)\n",
    "    df[\"content_strlen\"] = df.content.apply(len)\n",
    "#     df['level4_content_sub'] = df['level4_strlen'] - df['content_strlen']\n",
    "#     df['level4_content_div'] = df['level4_strlen'] / df['content_strlen']\n",
    "#     df[\"substr_level4_div\"] = df[\"content_level4_sublen\"] / df[\"level4_strlen\"]\n",
    "#     df[\"substr_content_div\"] = df[\"content_level4_sublen\"] / df[\"content_strlen\"]\n",
    "    \n",
    "    #------------------------- 子句 -------------------------#\n",
    "    df['level4_sent_num'] = df.level_4.apply(lambda x: get_len(x))\n",
    "    df['content_sent_num'] = df.content.apply(lambda x: get_len(x))\n",
    "    df['level4_content_sent_sub'] = df['level4_sent_num'] - df['content_sent_num']\n",
    "    \n",
    "    #------------------------- 分词 -------------------------#\n",
    "    # 基于jieba分词，计算三个特征：相同词的个数，不同词的个数，Jaccard相似度\n",
    "    df['level4_jieba'] = df['level_4'].apply(lambda x: set(jieba.cut(x)))\n",
    "    df['content_jieba'] = df['content'].apply(lambda x: set(jieba.cut(x)))\n",
    "    df['same_word'] = df.apply(lambda x: len(x.level4_jieba&x.content_jieba), axis=1)\n",
    "    df['different_word'] = df.apply(lambda x: len(x.content_jieba-x.level4_jieba), axis=1)\n",
    "    df['same_word_level4'] = df.apply(lambda x: x.same_word/len(x.level4_jieba), axis=1)\n",
    "    df['same_word_content'] = df.apply(lambda x: x.same_word/len(x.content_jieba) \n",
    "                                       if len(x.content_jieba)>0 else 0, axis=1)\n",
    "    df['jaccard'] = df.apply(lambda x: x.same_word/len(x.level4_jieba|x.content_jieba), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_content_features(df):\n",
    "    \"\"\"\"\"\"\n",
    "    #------------------------- 关键词特征 -------------------------#\n",
    "    # 对content分词，统计分词和标签\n",
    "    word_lst, label_lst = [], []\n",
    "    punctuation = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~“”？，！【】（）、。：；’‘……￥·1234567890 \"\"\"\n",
    "    df['jieba'] = df.apply(lambda x: [i for i,j in list(pseg.cut(x.content)) if i not in punctuation and \n",
    "                                       i not in x.content_level4_substr and j in [\"v\", \"d\", \"a\"]], axis=1)\n",
    "    for i, j in df[df.label.notna()][['jieba', 'label']].values:\n",
    "        word_lst.extend(i)\n",
    "        label_lst.extend(len(i)*[j])\n",
    "    df_word = pd.DataFrame({\"word\": word_lst, \"label\": label_lst})\n",
    "\n",
    "    word_label1_30 = list(df_word.loc[df_word.label==1, \"word\"].value_counts().sort_values(ascending=False)[:30].index)\n",
    "    word_label0_100 = set(df_word[df_word.label.notna()].word.value_counts().sort_values(ascending=False)[:130].index)-set(word_label1_30)\n",
    "    word_label1_30 = pickle.load(open(\"./data/ci_label1_30\", \"rb\"))\n",
    "    word_label0_100 = pickle.load(open(\"./data/ci_label0_100\", \"rb\"))\n",
    "    \n",
    "    for i, word in enumerate(word_label1_30):\n",
    "        df['wd_'+str(i)] = df.content.apply(lambda x: getIndex(word, x))  # 前30多的词，出现在当前content的索引\n",
    "    df[\"word_label0_100\"] = df.jieba.apply(lambda x: len(set(x)&word_label0_100))  # 前100正样本特有的词，出现在当前content几次\n",
    "    \n",
    "    #------------------------- textrank特征 -------------------------#\n",
    "    text0 = ' '.join(df[df.label==0].content.values)\n",
    "    text1 = ' '.join(df[df.label==1].content.values)\n",
    "    df_content_0 = get_word_weight(text0, nums=100, window=6)\n",
    "    df_content_1 = get_word_weight(text1, nums=100, window=6)\n",
    "    df_content_0.columns = ['word', 'weight_0']\n",
    "    df_content_1.columns = ['word', 'weight_1']\n",
    "    df_content_0_wd = pd.merge(df_content_0, df_content_1, on='word', how='left')\n",
    "    content_0_wd = df_content_0_wd[df_content_0_wd.weight_1.isna()]['word'].values\n",
    "    df_content_1_wd = pd.merge(df_content_1, df_content_0, on='word', how='left')\n",
    "    content_1_wd = df_content_1_wd[df_content_1_wd.weight_0.isna()]['word'].values\n",
    "\n",
    "    def get_main_wd_num(content, words):\n",
    "        word_sum = 0\n",
    "        for word in words:\n",
    "            if word in content:\n",
    "                word_sum += 1\n",
    "        return word_sum\n",
    "\n",
    "    df['content_0_cnt'] = df.content.apply(lambda x: get_main_wd_num(x, content_0_wd))\n",
    "    df['content_1_cnt'] = df.content.apply(lambda x: get_main_wd_num(x, content_1_wd))\n",
    "    \n",
    "    df_wd_1 = pd.DataFrame()\n",
    "    for i, word in enumerate(content_1_wd):\n",
    "        df_wd_1['prwd1_' + str(i)] = df.content.apply(lambda x: int(word in x))\n",
    "\n",
    "    df_wd_0 = pd.DataFrame()\n",
    "    for i, word in enumerate(content_0_wd):\n",
    "        df_wd_0['prwd0_' + str(i)] = df.content.apply(lambda x: int(word in x))\n",
    "\n",
    "    ## 特征筛选\n",
    "    # 方法一：编码后降维\n",
    "    pca = PCA(n_components=0.8)\n",
    "    pca.fit(df_wd_1)\n",
    "    wd1_pca = pca.transform(df_wd_1)\n",
    "\n",
    "    for i in range(wd1_pca.shape[1]):\n",
    "        df[f'content_pr1_{i}'] = wd1_pca[:, i]\n",
    "\n",
    "    pca = PCA(n_components=0.8)\n",
    "    pca.fit(df_wd_0)\n",
    "    wd0_pca = pca.transform(df_wd_0)\n",
    "\n",
    "    for i in range(wd0_pca.shape[1]):\n",
    "        df[f'content_pr0_{i}'] = wd0_pca[:, i]\n",
    "\n",
    "    # 方法二：选择IV大于0.02的分词，降维后进入模型\n",
    "    # df_wd_1['label'] = df.label\n",
    "    # df_wd_0['label'] = df.label\n",
    "    # df_wd1_iv = toad.quality(df_wd_1, target='label')\n",
    "    # df_wd1_iv_sel = df_wd1_iv[df_wd1_iv.iv > 0.02].index.values\n",
    "    # df_wd0_iv = toad.quality(df_wd_0, target='label')\n",
    "    # df_wd0_iv_sel = df_wd0_iv[df_wd0_iv.iv > 0.02].index.values\n",
    "\n",
    "    # pca = PCA(n_components=0.95)\n",
    "    # pca.fit(df_wd_1[df_wd1_iv_sel])\n",
    "    # wd1_pca = pca.transform(df_wd_1[df_wd1_iv_sel])\n",
    "\n",
    "    # for i in range(wd1_pca.shape[1]):\n",
    "    #     df[f'content_pr1_{i}'] = wd1_pca[:, i]\n",
    "\n",
    "    # pca = PCA(n_components=0.8)\n",
    "    # pca.fit(df_wd_0[df_wd0_iv_sel])\n",
    "    # wd0_pca = pca.transform(df_wd_0[df_wd0_iv_sel])\n",
    "\n",
    "    # for i in range(wd0_pca.shape[1]):\n",
    "    #     df[f'content_pr0_{i}'] = wd0_pca[:, i]\n",
    "    \n",
    "    #------------------------- TF-IDF特征 -------------------------#\n",
    "    df['content_seg'] = df['content'].apply(lambda x: \" \".join(jieba.cut(x)))\n",
    "    df['content_word_cnt'] = df['content_seg'].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "    n_components = 16\n",
    "    X = list(df['content_seg'].values)\n",
    "    tfv = TfidfVectorizer(ngram_range=(1,1), \n",
    "                          token_pattern=r\"(?u)\\b[^ ]+\\b\",\n",
    "                          max_features=10000)\n",
    "    tfv.fit(X)\n",
    "    X_tfidf = tfv.transform(X)\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    svd.fit(X_tfidf)\n",
    "    X_svd = svd.transform(X_tfidf)\n",
    "\n",
    "    for i in range(n_components):\n",
    "        df[f'content_tfidf_{i}'] = X_svd[:, i]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def get_level_features(df):\n",
    "    #------------------------- badrate编码 -------------------------#\n",
    "    for col in ['level_1', 'level_2', 'level_3']:\n",
    "        risk_ratio = dict(df[df.label.notna()].groupby(col)['label'].mean())\n",
    "        df[f'{col}_risk_score'] = df[col].map(risk_ratio)\n",
    "    \n",
    "    #------------------------- 类别编码 -------------------------#\n",
    "    for col in ['level_1', 'level_2', 'level_3', 'level_4']:\n",
    "        df[f'{col}_strlen'] = df[col].astype(str).apply(len)\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(df[col])\n",
    "        df[col] = lbl.transform(df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_bert_features(train, test):\n",
    "    # bert\n",
    "    train_bert_pred = pd.read_csv('./data/roberta_pred_oof2.csv')\n",
    "    test_bert_pred = pd.read_csv('./data/roberta_pred_test2.csv')\n",
    "\n",
    "    train = pd.merge(train, train_bert_pred, on='id')\n",
    "    test = pd.merge(test, test_bert_pred, on='id')\n",
    "    \n",
    "    # sentence pair bert\n",
    "    train_sbert_pred = pd.read_csv('./data/roberta2_pred_oof.csv')\n",
    "    test_sbert_pred = pd.read_csv('./data/roberta2_pred_test.csv')\n",
    "\n",
    "    train = pd.merge(train, train_sbert_pred, on='id')\n",
    "    test = pd.merge(test, test_sbert_pred, on='id')\n",
    "    \n",
    "    # 直接用预训练模型encode，计算文本相似度\n",
    "    df_tr_sim = pd.read_csv('./data/train_sim.csv')\n",
    "    df_ts_sim = pd.read_csv('./data/test_sim.csv')\n",
    "\n",
    "    train = pd.concat([train, df_tr_sim], axis=1)\n",
    "    test = pd.concat([test, df_ts_sim], axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "def mainProcess(train_path, test_path):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    train['content'].fillna('', inplace=True)\n",
    "    test['content'].fillna('', inplace=True)\n",
    "    train['type'] = 'train'\n",
    "    test['type'] = 'test'\n",
    "    df = pd.concat([train, test], ignore_index=True)\n",
    "    df = get_level_content_features(df)\n",
    "    df = get_content_features(df)\n",
    "    df = get_level_features(df)\n",
    "    drop_cols = ['content', 'content_nosign', 'level4_nosign', 'content_level4_substr', 'level4_jieba','content_jieba', \n",
    "                 'jieba', 'content_seg', 'type']\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "    train = df[df['label'].notna()]\n",
    "    test = df[df['label'].isna()]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、自动调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hp_param(train_df):\n",
    "    \"\"\"基于贝叶斯的自动调参\"\"\"\n",
    "    data = train_df\n",
    "    X = data.drop(['id', 'label'], axis=1)\n",
    "    y = data['label']\n",
    "\n",
    "    #split\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    train_data = lgb.Dataset(data=X_train,label=y_train)\n",
    "    test_data = lgb.Dataset(data=X_test,label=y_test)\n",
    "    \n",
    "    # 搜索空间\n",
    "    space = {\"max_depth\": hp.randint(\"max_depth\", 15),\n",
    "             \"num_trees\": hp.randint(\"num_trees\", 300),\n",
    "             \"learning_rate\": hp.uniform(\"learning_rate\", 1e-3, 5e-1),\n",
    "             \"bagging_fraction\": hp.randint(\"bagging_fraction\", 5),\n",
    "             \"num_leaves\": hp.randint(\"num_leaves\", 6),   \n",
    "            }\n",
    "    \n",
    "    # 参数变换\n",
    "    def argsDict_tranform(argsDict, isPrint=False):\n",
    "        argsDict[\"max_depth\"] = argsDict[\"max_depth\"] + 5\n",
    "        argsDict['num_trees'] = argsDict['num_trees'] + 150\n",
    "        argsDict[\"learning_rate\"] = argsDict[\"learning_rate\"] * 0.02 + 0.05\n",
    "        argsDict[\"bagging_fraction\"] = argsDict[\"bagging_fraction\"] * 0.1 + 0.5\n",
    "        argsDict[\"num_leaves\"] = argsDict[\"num_leaves\"] * 3 + 10\n",
    "        if isPrint: \n",
    "            print(argsDict)\n",
    "        else:\n",
    "            pass \n",
    "        return argsDict\n",
    "    \n",
    "    def lgb_f1_score(y_hat, data):\n",
    "        y_true = data.get_label()\n",
    "        y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "        return 'f1', f1_score(y_true, y_hat), True\n",
    "    \n",
    "    # 模型生成器\n",
    "    def lightgbm_factory(argsDict):\n",
    "        argsDict = argsDict_tranform(argsDict)\n",
    "        \n",
    "        params={'nthread': -1,  # 进程数\n",
    "                'max_depth': argsDict['max_depth'],  # 最大深度\n",
    "                'num_trees': argsDict['num_trees'],  # 树的数量\n",
    "                'eta': argsDict['learning_rate'],    # 学习率\n",
    "                'bagging_fraction': argsDict['bagging_fraction'],  # 样本采样\n",
    "                'num_leaves': argsDict['num_leaves'],  # 终点节点最小样本占比的和\n",
    "                'objective': 'binary',\n",
    "                'feature_fraction': 0.8,  # 特征采样\n",
    "                'lambda_11': 2,  # L1正则化\n",
    "                'lambda_12': 3,  # L2正则化\n",
    "                'baggingseed': 100,  # 随机种子，默认为100\n",
    "               }\n",
    "        params['metric'] = ['auc']\n",
    "\n",
    "\n",
    "        model_lgb = lgb.train(params, train_data, valid_sets=[test_data], feval=lgb_f1_score, early_stopping_rounds=10)\n",
    "        return get_tranformer_score(model_lgb)\n",
    "    \n",
    "    # 获取损失函数\n",
    "    def get_tranformer_score(tranformer):\n",
    "        model = tranformer\n",
    "        prediction = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        return -sklearn.metrics.roc_auc_score(y_test, prediction)\n",
    "    \n",
    "    # 开始调参\n",
    "    best = fmin(lightgbm_factory, space, algo=tpe.suggest, max_evals=6)\n",
    "    print('best:')\n",
    "    print(best)\n",
    "    \n",
    "    # 得到最佳参数\n",
    "    print('best param')\n",
    "    params = argsDict_tranform(best, isPrint=True)\n",
    "    return params\n",
    "    \n",
    "def model_lgb_hp(train_df, params):\n",
    "    data = train_df\n",
    "    X = data.drop(['id', 'label'], axis=1)\n",
    "    y = data['label']\n",
    "\n",
    "    #split\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    train_data = lgb.Dataset(data=X_train,label=y_train)\n",
    "    test_data = lgb.Dataset(data=X_test,label=y_test)\n",
    "\n",
    "    #train\n",
    "    gbm_model = LGBMClassifier(boosting_type='gbdt', **params)\n",
    "    gbm_model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric='AUC', early_stopping_rounds=10)\n",
    "    \n",
    "    pickle.dump(gbm_model, open(\"model\", \"wb\"))\n",
    "    print(\"lgb已保存成文件model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.98658\tvalid_0's f1: 0                                                                             \n",
      "Training until validation scores don't improve for 10 rounds                                                           \n",
      "[2]\tvalid_0's auc: 0.987391\tvalid_0's f1: 0                                                                            \n",
      "[3]\tvalid_0's auc: 0.987538\tvalid_0's f1: 0                                                                            \n",
      "[4]\tvalid_0's auc: 0.987576\tvalid_0's f1: 0                                                                            \n",
      "[5]\tvalid_0's auc: 0.98756\tvalid_0's f1: 0                                                                             \n",
      "[6]\tvalid_0's auc: 0.987569\tvalid_0's f1: 0                                                                            \n",
      "[7]\tvalid_0's auc: 0.987544\tvalid_0's f1: 0                                                                            \n",
      "[8]\tvalid_0's auc: 0.987586\tvalid_0's f1: 0                                                                            \n",
      "[9]\tvalid_0's auc: 0.987579\tvalid_0's f1: 0                                                                            \n",
      "[10]\tvalid_0's auc: 0.987592\tvalid_0's f1: 0.0383142                                                                   \n",
      "[11]\tvalid_0's auc: 0.987592\tvalid_0's f1: 0.860262                                                                    \n",
      "[12]\tvalid_0's auc: 0.987577\tvalid_0's f1: 0.880342                                                                    \n",
      "[13]\tvalid_0's auc: 0.987614\tvalid_0's f1: 0.885106                                                                    \n",
      "[14]\tvalid_0's auc: 0.987633\tvalid_0's f1: 0.89916                                                                     \n",
      "[15]\tvalid_0's auc: 0.987653\tvalid_0's f1: 0.906054                                                                    \n",
      "[16]\tvalid_0's auc: 0.987556\tvalid_0's f1: 0.908333                                                                    \n",
      "[17]\tvalid_0's auc: 0.987319\tvalid_0's f1: 0.913223                                                                    \n",
      "[18]\tvalid_0's auc: 0.987343\tvalid_0's f1: 0.915464                                                                    \n",
      "[19]\tvalid_0's auc: 0.987343\tvalid_0's f1: 0.915811                                                                    \n",
      "[20]\tvalid_0's auc: 0.987367\tvalid_0's f1: 0.922449                                                                    \n",
      "[21]\tvalid_0's auc: 0.987341\tvalid_0's f1: 0.924644                                                                    \n",
      "[22]\tvalid_0's auc: 0.987334\tvalid_0's f1: 0.924644                                                                    \n",
      "[23]\tvalid_0's auc: 0.987374\tvalid_0's f1: 0.924644                                                                    \n",
      "[24]\tvalid_0's auc: 0.987336\tvalid_0's f1: 0.922764                                                                    \n",
      "[25]\tvalid_0's auc: 0.987363\tvalid_0's f1: 0.922764                                                                    \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[15]\tvalid_0's auc: 0.987653\tvalid_0's f1: 0.906054\n",
      "[1]\tvalid_0's auc: 0.983079\tvalid_0's f1: 0                                                                            \n",
      "Training until validation scores don't improve for 10 rounds                                                           \n",
      "[2]\tvalid_0's auc: 0.980022\tvalid_0's f1: 0                                                                            \n",
      "[3]\tvalid_0's auc: 0.98745\tvalid_0's f1: 0                                                                             \n",
      "[4]\tvalid_0's auc: 0.987546\tvalid_0's f1: 0                                                                            \n",
      "[5]\tvalid_0's auc: 0.987395\tvalid_0's f1: 0                                                                            \n",
      "[6]\tvalid_0's auc: 0.987428\tvalid_0's f1: 0                                                                            \n",
      "[7]\tvalid_0's auc: 0.987416\tvalid_0's f1: 0                                                                            \n",
      "[8]\tvalid_0's auc: 0.987446\tvalid_0's f1: 0                                                                            \n",
      "[9]\tvalid_0's auc: 0.987399\tvalid_0's f1: 0                                                                            \n",
      "[10]\tvalid_0's auc: 0.987459\tvalid_0's f1: 0.85022                                                                     \n",
      "[11]\tvalid_0's auc: 0.987424\tvalid_0's f1: 0.873118                                                                    \n",
      "[12]\tvalid_0's auc: 0.987397\tvalid_0's f1: 0.882729                                                                    \n",
      "[13]\tvalid_0's auc: 0.987409\tvalid_0's f1: 0.892178                                                                    \n",
      "[14]\tvalid_0's auc: 0.987413\tvalid_0's f1: 0.901879                                                                    \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[4]\tvalid_0's auc: 0.987546\tvalid_0's f1: 0\n",
      "[1]\tvalid_0's auc: 0.982898\tvalid_0's f1: 0                                                                            \n",
      "Training until validation scores don't improve for 10 rounds                                                           \n",
      "[2]\tvalid_0's auc: 0.987153\tvalid_0's f1: 0                                                                            \n",
      "[3]\tvalid_0's auc: 0.987287\tvalid_0's f1: 0                                                                            \n",
      "[4]\tvalid_0's auc: 0.987311\tvalid_0's f1: 0                                                                            \n",
      "[5]\tvalid_0's auc: 0.987737\tvalid_0's f1: 0                                                                            \n",
      "[6]\tvalid_0's auc: 0.987695\tvalid_0's f1: 0                                                                            \n",
      "[7]\tvalid_0's auc: 0.987643\tvalid_0's f1: 0                                                                            \n",
      "[8]\tvalid_0's auc: 0.987656\tvalid_0's f1: 0                                                                            \n",
      "[9]\tvalid_0's auc: 0.987655\tvalid_0's f1: 0                                                                            \n",
      "[10]\tvalid_0's auc: 0.987612\tvalid_0's f1: 0                                                                           \n",
      "[11]\tvalid_0's auc: 0.987635\tvalid_0's f1: 0.768496                                                                    \n",
      "[12]\tvalid_0's auc: 0.98761\tvalid_0's f1: 0.877944                                                                     \n",
      "[13]\tvalid_0's auc: 0.987606\tvalid_0's f1: 0.880342                                                                    \n",
      "[14]\tvalid_0's auc: 0.987599\tvalid_0's f1: 0.89916                                                                     \n",
      "[15]\tvalid_0's auc: 0.987599\tvalid_0's f1: 0.903766                                                                    \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[5]\tvalid_0's auc: 0.987737\tvalid_0's f1: 0\n",
      "[1]\tvalid_0's auc: 0.986561\tvalid_0's f1: 0                                                                            \n",
      "Training until validation scores don't improve for 10 rounds                                                           \n",
      "[2]\tvalid_0's auc: 0.986764\tvalid_0's f1: 0                                                                            \n",
      "[3]\tvalid_0's auc: 0.987022\tvalid_0's f1: 0                                                                            \n",
      "[4]\tvalid_0's auc: 0.987139\tvalid_0's f1: 0                                                                            \n",
      "[5]\tvalid_0's auc: 0.986743\tvalid_0's f1: 0                                                                            \n",
      "[6]\tvalid_0's auc: 0.986537\tvalid_0's f1: 0                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\tvalid_0's auc: 0.986798\tvalid_0's f1: 0                                                                            \n",
      "[8]\tvalid_0's auc: 0.986875\tvalid_0's f1: 0                                                                            \n",
      "[9]\tvalid_0's auc: 0.986856\tvalid_0's f1: 0                                                                            \n",
      "[10]\tvalid_0's auc: 0.986791\tvalid_0's f1: 0.751807                                                                    \n",
      "[11]\tvalid_0's auc: 0.986754\tvalid_0's f1: 0.860262                                                                    \n",
      "[12]\tvalid_0's auc: 0.986855\tvalid_0's f1: 0.879828                                                                    \n",
      "[13]\tvalid_0's auc: 0.9869\tvalid_0's f1: 0.886994                                                                      \n",
      "[14]\tvalid_0's auc: 0.986915\tvalid_0's f1: 0.894068                                                                    \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[4]\tvalid_0's auc: 0.987139\tvalid_0's f1: 0\n",
      "[1]\tvalid_0's auc: 0.986561\tvalid_0's f1: 0                                                                            \n",
      "Training until validation scores don't improve for 10 rounds                                                           \n",
      "[2]\tvalid_0's auc: 0.986764\tvalid_0's f1: 0                                                                            \n",
      "[3]\tvalid_0's auc: 0.987022\tvalid_0's f1: 0                                                                            \n",
      "[4]\tvalid_0's auc: 0.987139\tvalid_0's f1: 0                                                                            \n",
      "[5]\tvalid_0's auc: 0.986743\tvalid_0's f1: 0                                                                            \n",
      "[6]\tvalid_0's auc: 0.986537\tvalid_0's f1: 0                                                                            \n",
      "[7]\tvalid_0's auc: 0.986798\tvalid_0's f1: 0                                                                            \n",
      "[8]\tvalid_0's auc: 0.986875\tvalid_0's f1: 0                                                                            \n",
      "[9]\tvalid_0's auc: 0.986856\tvalid_0's f1: 0                                                                            \n",
      "[10]\tvalid_0's auc: 0.986791\tvalid_0's f1: 0.751807                                                                    \n",
      "[11]\tvalid_0's auc: 0.986756\tvalid_0's f1: 0.860262                                                                    \n",
      "[12]\tvalid_0's auc: 0.986838\tvalid_0's f1: 0.879828                                                                    \n",
      "[13]\tvalid_0's auc: 0.986878\tvalid_0's f1: 0.889362                                                                    \n",
      "[14]\tvalid_0's auc: 0.98688\tvalid_0's f1: 0.894068                                                                     \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[4]\tvalid_0's auc: 0.987139\tvalid_0's f1: 0\n",
      "[1]\tvalid_0's auc: 0.986454\tvalid_0's f1: 0                                                                            \n",
      "Training until validation scores don't improve for 10 rounds                                                           \n",
      "[2]\tvalid_0's auc: 0.986942\tvalid_0's f1: 0                                                                            \n",
      "[3]\tvalid_0's auc: 0.987396\tvalid_0's f1: 0                                                                            \n",
      "[4]\tvalid_0's auc: 0.987507\tvalid_0's f1: 0                                                                            \n",
      "[5]\tvalid_0's auc: 0.987471\tvalid_0's f1: 0                                                                            \n",
      "[6]\tvalid_0's auc: 0.987382\tvalid_0's f1: 0                                                                            \n",
      "[7]\tvalid_0's auc: 0.987362\tvalid_0's f1: 0                                                                            \n",
      "[8]\tvalid_0's auc: 0.987411\tvalid_0's f1: 0                                                                            \n",
      "[9]\tvalid_0's auc: 0.987409\tvalid_0's f1: 0                                                                            \n",
      "[10]\tvalid_0's auc: 0.987287\tvalid_0's f1: 0.817352                                                                    \n",
      "[11]\tvalid_0's auc: 0.987289\tvalid_0's f1: 0.867679                                                                    \n",
      "[12]\tvalid_0's auc: 0.987262\tvalid_0's f1: 0.884615                                                                    \n",
      "[13]\tvalid_0's auc: 0.987302\tvalid_0's f1: 0.896406                                                                    \n",
      "[14]\tvalid_0's auc: 0.987302\tvalid_0's f1: 0.901053                                                                    \n",
      "Early stopping, best iteration is:                                                                                     \n",
      "[4]\tvalid_0's auc: 0.987507\tvalid_0's f1: 0\n",
      "100%|█████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.08trial/s, best loss: -0.9877373994286379]\n",
      "best:\n",
      "{'bagging_fraction': 3, 'learning_rate': 0.0652648821436768, 'max_depth': 14, 'num_leaves': 0, 'num_trees': 211}\n",
      "best param\n",
      "{'bagging_fraction': 0.8, 'learning_rate': 0.05130529764287354, 'max_depth': 19, 'num_leaves': 10, 'num_trees': 361}\n"
     ]
    }
   ],
   "source": [
    "params = hp_param(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.983624\ttraining's binary_logloss: 0.297589\tvalid_1's auc: 0.982898\tvalid_1's binary_logloss: 0.296837\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\ttraining's auc: 0.983811\ttraining's binary_logloss: 0.268301\tvalid_1's auc: 0.98312\tvalid_1's binary_logloss: 0.268148\n",
      "[3]\ttraining's auc: 0.984603\ttraining's binary_logloss: 0.245756\tvalid_1's auc: 0.983442\tvalid_1's binary_logloss: 0.246082\n",
      "[4]\ttraining's auc: 0.986076\ttraining's binary_logloss: 0.227237\tvalid_1's auc: 0.985464\tvalid_1's binary_logloss: 0.228082\n",
      "[5]\ttraining's auc: 0.986119\ttraining's binary_logloss: 0.211596\tvalid_1's auc: 0.98552\tvalid_1's binary_logloss: 0.212737\n",
      "[6]\ttraining's auc: 0.98616\ttraining's binary_logloss: 0.198005\tvalid_1's auc: 0.985513\tvalid_1's binary_logloss: 0.1996\n",
      "[7]\ttraining's auc: 0.98618\ttraining's binary_logloss: 0.186025\tvalid_1's auc: 0.985569\tvalid_1's binary_logloss: 0.187731\n",
      "[8]\ttraining's auc: 0.986204\ttraining's binary_logloss: 0.175366\tvalid_1's auc: 0.985591\tvalid_1's binary_logloss: 0.177347\n",
      "[9]\ttraining's auc: 0.986217\ttraining's binary_logloss: 0.1658\tvalid_1's auc: 0.985589\tvalid_1's binary_logloss: 0.168108\n",
      "[10]\ttraining's auc: 0.986216\ttraining's binary_logloss: 0.157106\tvalid_1's auc: 0.987472\tvalid_1's binary_logloss: 0.159522\n",
      "[11]\ttraining's auc: 0.990248\ttraining's binary_logloss: 0.149133\tvalid_1's auc: 0.987348\tvalid_1's binary_logloss: 0.15183\n",
      "[12]\ttraining's auc: 0.990259\ttraining's binary_logloss: 0.141857\tvalid_1's auc: 0.987345\tvalid_1's binary_logloss: 0.144772\n",
      "[13]\ttraining's auc: 0.993033\ttraining's binary_logloss: 0.135154\tvalid_1's auc: 0.987335\tvalid_1's binary_logloss: 0.138399\n",
      "[14]\ttraining's auc: 0.992899\ttraining's binary_logloss: 0.128942\tvalid_1's auc: 0.987091\tvalid_1's binary_logloss: 0.13239\n",
      "[15]\ttraining's auc: 0.992962\ttraining's binary_logloss: 0.123221\tvalid_1's auc: 0.98708\tvalid_1's binary_logloss: 0.126988\n",
      "[16]\ttraining's auc: 0.992962\ttraining's binary_logloss: 0.117913\tvalid_1's auc: 0.987076\tvalid_1's binary_logloss: 0.121801\n",
      "[17]\ttraining's auc: 0.992956\ttraining's binary_logloss: 0.112987\tvalid_1's auc: 0.987081\tvalid_1's binary_logloss: 0.117043\n",
      "[18]\ttraining's auc: 0.992993\ttraining's binary_logloss: 0.108375\tvalid_1's auc: 0.987054\tvalid_1's binary_logloss: 0.112737\n",
      "[19]\ttraining's auc: 0.99304\ttraining's binary_logloss: 0.104058\tvalid_1's auc: 0.987102\tvalid_1's binary_logloss: 0.108594\n",
      "[20]\ttraining's auc: 0.993068\ttraining's binary_logloss: 0.100044\tvalid_1's auc: 0.987096\tvalid_1's binary_logloss: 0.104871\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's auc: 0.986216\ttraining's binary_logloss: 0.157106\tvalid_1's auc: 0.987472\tvalid_1's binary_logloss: 0.159522\n",
      "lgb已保存成文件model\n"
     ]
    }
   ],
   "source": [
    "model_lgb_hp(train, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, test):\n",
    "    ycol = 'label'\n",
    "    feature_names = list(filter(lambda x: x not in [ycol, 'id'], train.columns))\n",
    "\n",
    "    # 贝叶斯调参 A榜f1: 0.95887806\n",
    "    model = lgb.LGBMClassifier(objective='binary',\n",
    "                               boosting_type='gbdt',\n",
    "                               learning_rate=0.05,\n",
    "                               n_estimators=1000,\n",
    "                               max_depth=15,\n",
    "                               num_leaves=13,\n",
    "                               subsample=0.6,\n",
    "                               feature_fraction=0.8,\n",
    "                               reg_alpha=2,     \n",
    "                               reg_lambda=3,    \n",
    "                               random_state=2021,\n",
    "                               is_unbalance=True,\n",
    "                               metric='auc')\n",
    "\n",
    "    oof = []\n",
    "    prediction = test[['id']]\n",
    "    prediction[ycol] = 0\n",
    "    df_importance_list = []\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "    for fold_id, (trn_idx, val_idx) in enumerate(kfold.split(train[feature_names], train[ycol])):\n",
    "        X_train = train.iloc[trn_idx][feature_names]\n",
    "        Y_train = train.iloc[trn_idx][ycol]\n",
    "\n",
    "        X_val = train.iloc[val_idx][feature_names]\n",
    "        Y_val = train.iloc[val_idx][ycol]\n",
    "\n",
    "        print('\\nFold_{} Training ================================\\n'.format(fold_id+1))\n",
    "\n",
    "        lgb_model = model.fit(X_train,\n",
    "                              Y_train,\n",
    "                              eval_names=['train', 'valid'],\n",
    "                              eval_set=[(X_train, Y_train), (X_val, Y_val)],\n",
    "                              verbose=100,\n",
    "                              eval_metric='auc',\n",
    "                              early_stopping_rounds=50)\n",
    "\n",
    "        pred_val = lgb_model.predict_proba(X_val, num_iteration=lgb_model.best_iteration_)\n",
    "        df_oof = train.iloc[val_idx][['id', ycol]].copy()\n",
    "        df_oof['pred'] = pred_val[:,1]\n",
    "        oof.append(df_oof)\n",
    "\n",
    "        pred_test = lgb_model.predict_proba(test[feature_names], num_iteration=lgb_model.best_iteration_)\n",
    "        prediction[ycol] += pred_test[:,1] / kfold.n_splits\n",
    "\n",
    "        df_importance = pd.DataFrame({\n",
    "            'column': feature_names,\n",
    "            'importance': lgb_model.feature_importances_,\n",
    "        })\n",
    "        df_importance_list.append(df_importance)\n",
    "\n",
    "        del lgb_model, pred_val, pred_test, X_train, Y_train, X_val, Y_val\n",
    "        gc.collect()\n",
    "    \n",
    "    df_oof = pd.concat(oof)\n",
    "    df_importance = pd.concat(df_importance_list)\n",
    "    df_importance = df_importance.groupby(['column'])['importance'].agg(\n",
    "        'mean').sort_values(ascending=False).reset_index()\n",
    "    print(df_importance)\n",
    "    \n",
    "    return df_oof, prediction, df_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、寻找最优切分点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_split(df_oof, prediction):\n",
    "    i_bst = 0\n",
    "    bst = 0\n",
    "    for i in np.arange(0.1, 1, 0.05):\n",
    "        df_oof['pred_label'] = df_oof['pred'].apply(lambda x: 1 if x >= i else 0)\n",
    "        score = f1_score(df_oof['label'], df_oof['pred_label'])\n",
    "        print(i, 'f1_score:', score)\n",
    "        if score> bst:\n",
    "            i_bst = i\n",
    "            bst = score\n",
    "    print('best split point: {}, best f1-score: {}'.format(i_bst, bst))\n",
    "    \n",
    "    prediction['label'] = prediction['label'].apply(lambda x: 1 if x >= i_bst else 0)\n",
    "    print(prediction['label'].value_counts())\n",
    "    return prediction[['id', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据预处理...\n",
      "训练模型...\n",
      "\n",
      "Fold_1 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.998856\tvalid's auc: 0.992418\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttrain's auc: 0.999204\tvalid's auc: 0.992803\n",
      "\n",
      "Fold_2 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.99881\tvalid's auc: 0.994931\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttrain's auc: 0.998795\tvalid's auc: 0.995108\n",
      "\n",
      "Fold_3 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.998976\tvalid's auc: 0.985797\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttrain's auc: 0.998004\tvalid's auc: 0.986633\n",
      "\n",
      "Fold_4 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.999213\tvalid's auc: 0.988753\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttrain's auc: 0.999027\tvalid's auc: 0.989578\n",
      "\n",
      "Fold_5 Training ================================\n",
      "\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's auc: 0.998795\tvalid's auc: 0.994695\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttrain's auc: 0.998359\tvalid's auc: 0.995042\n",
      "               column  importance\n",
      "0         bert_pred_y       127.0\n",
      "1         bert_pred_x       122.8\n",
      "2    content_tfidf_11        38.2\n",
      "3     content_tfidf_0        37.8\n",
      "4    content_tfidf_10        34.6\n",
      "..                ...         ...\n",
      "107              wd_4         0.0\n",
      "108             wd_25         0.0\n",
      "109             wd_29         0.0\n",
      "110             wd_28         0.0\n",
      "111             wd_19         0.0\n",
      "\n",
      "[112 rows x 2 columns]\n",
      "0.1 f1_score: 0.7603853100541843\n",
      "0.15000000000000002 f1_score: 0.8142394822006473\n",
      "0.20000000000000004 f1_score: 0.8392498325519089\n",
      "0.25000000000000006 f1_score: 0.8588114050154587\n",
      "0.30000000000000004 f1_score: 0.8713286713286713\n",
      "0.3500000000000001 f1_score: 0.8830616583982991\n",
      "0.40000000000000013 f1_score: 0.8921533500537442\n",
      "0.45000000000000007 f1_score: 0.8991291727140784\n",
      "0.5000000000000001 f1_score: 0.907957462412908\n",
      "0.5500000000000002 f1_score: 0.9113082039911308\n",
      "0.6000000000000002 f1_score: 0.9134328358208955\n",
      "0.6500000000000001 f1_score: 0.9170731707317074\n",
      "0.7000000000000002 f1_score: 0.919000757002271\n",
      "0.7500000000000002 f1_score: 0.9223115193264447\n",
      "0.8000000000000002 f1_score: 0.9242718446601942\n",
      "0.8500000000000002 f1_score: 0.9224714679260133\n",
      "0.9000000000000002 f1_score: 0.9174385823600484\n",
      "0.9500000000000003 f1_score: 0.797088262056415\n",
      "best split point: 0.8000000000000002, best f1-score: 0.9242718446601942\n",
      "0    16082\n",
      "1     1918\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_path = \"./data/train.csv\"\n",
    "    test_path = \"./data/test.csv\"\n",
    "    \n",
    "    print(\"数据预处理...\")\n",
    "    train, test = mainProcess(train_path, test_path)\n",
    "    train, test = get_bert_features(train, test)\n",
    "    print(\"训练模型...\")\n",
    "    df_oof, prediction, df_importance = train_model(train, test)\n",
    "    result = search_best_split(df_oof, prediction)\n",
    "    result.to_csv(f'./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert_pred_y</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bert_pred_x</td>\n",
       "      <td>122.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_11</td>\n",
       "      <td>38.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>content_tfidf_0</td>\n",
       "      <td>37.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>content_tfidf_10</td>\n",
       "      <td>34.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>level_3_risk_score</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l1_sim</td>\n",
       "      <td>29.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>content_tfidf_3</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>content_tfidf_7</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>content_strlen</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>l4_sim</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>different_word</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>content_tfidf_15</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>content_tfidf_9</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>content_pr0_6</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>l3_sim</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>content_tfidf_4</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>content_tfidf_6</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>content_tfidf_12</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>l2_sim</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                column  importance\n",
       "0          bert_pred_y       127.0\n",
       "1          bert_pred_x       122.8\n",
       "2     content_tfidf_11        38.2\n",
       "3      content_tfidf_0        37.8\n",
       "4     content_tfidf_10        34.6\n",
       "5   level_3_risk_score        30.6\n",
       "6               l1_sim        29.4\n",
       "7      content_tfidf_3        26.0\n",
       "8      content_tfidf_7        22.2\n",
       "9       content_strlen        21.6\n",
       "10              l4_sim        21.2\n",
       "11      different_word        20.4\n",
       "12    content_tfidf_15        19.6\n",
       "13     content_tfidf_9        19.2\n",
       "14       content_pr0_6        17.8\n",
       "15              l3_sim        17.0\n",
       "16     content_tfidf_4        16.8\n",
       "17     content_tfidf_6        15.6\n",
       "18    content_tfidf_12        15.2\n",
       "19              l2_sim        15.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_importance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   0      0\n",
       "1   1      0\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
